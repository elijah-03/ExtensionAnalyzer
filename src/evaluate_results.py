#!/usr/bin/env python3

"""
Analyzes the 'results.csv' file generated by 'run_evaluation.py'
to find the optimal threat score threshold for classifying extensions.

Requires:
- pandas: (pip install pandas)
- scikit-learn: (pip install scikit-learn)
"""

import os
import pandas as pd
from sklearn.metrics import precision_recall_fscore_support, confusion_matrix

# --- Configuration ---
# Get the directory where this script is located (e.g., /.../src)
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
# Get the base directory (e.g., /.../ExtensionAnalyzer)
BASE_DIR = os.path.dirname(SCRIPT_DIR)

# Path to the results.csv file
CSV_PATH = os.path.join(BASE_DIR, 'results.csv')
# ---------------------

def analyze_results(df):
    """
    Calculates precision, recall, and f1-score for all possible thresholds.
    
    Args:
        df (pd.DataFrame): The DataFrame containing 'threat_score' and 'label'.
    """
    
    # Prepare the true labels (1 for malicious, 0 for benign)
    y_true = (df['label'] == 'malicious').astype(int)
    
    # Get all unique scores to test as thresholds.
    # We test at score + 1 so that a threshold of 10 flags scores >= 10
    thresholds = sorted(df['threat_score'].unique())
    if not thresholds:
        print("No scores found to analyze.")
        return

    print(f"Analyzing {len(y_true)} extensions across {len(thresholds)} unique score thresholds...")
    
    best_f1 = -1
    best_threshold = -1
    best_metrics = {}

    # Test every score as a potential threshold
    # We also add a threshold just above the max score to see results for "flag nothing"
    for threshold in thresholds + [thresholds[-1] + 1]:
        
        # Predict "malicious" (1) if the score is >= the threshold
        y_pred = (df['threat_score'] >= threshold).astype(int)
        
        # Calculate metrics
        precision, recall, f1, _ = precision_recall_fscore_support(
            y_true, y_pred, average='binary', pos_label=1, zero_division=0
        )
        
        # Check if this is the best F1-score so far
        if f1 > best_f1:
            best_f1 = f1
            best_threshold = threshold
            tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
            best_metrics = {
                'precision': precision,
                'recall': recall,
                'f1_score': f1,
                'threshold': threshold,
                'tp': tp, # True Positives (Malicious correctly flagged)
                'fn': fn, # False Negatives (Malicious missed)
                'fp': fp, # False Positives (Benign incorrectly flagged)
                'tn': tn  # True Negatives (Benign correctly ignored)
            }
        
    # --- Print the final report ---
    print("\n" + "="*40)
    print("Optimal Threshold Analysis")
    print("="*40)
    
    if best_f1 == 0:
        print("Warning: Could not find a useful threshold (F1-score is 0).")
        print("This may mean no malicious extensions were detected at any threshold.")
        return

    m = best_metrics
    print(f"Best F1-Score:     {m['f1_score']:.4f}")
    print(f"Best Threshold:    >= {m['threshold']} (scores of {m['threshold']} or higher are flagged as malicious)")
    print("\n--- Metrics at this threshold ---")
    print(f"Precision:         {m['precision']:.4f} (Of all flagged, {(m['precision'] * 100):.1f}% were truly malicious)")
    print(f"Recall:            {m['recall']:.4f} (Correctly flagged {m['tp']} out of {m['tp'] + m['fn']} malicious extensions)")
    
    print("\n--- Confusion Matrix ---")
    print("                 Predicted:")
    print("                 Benign  | Malicious")
    print("-----------------------------------")
    print(f"Actual: Benign   |  {m['tn']:<5}   |  {m['fp']:<5}  (False Positives)")
    print(f"Actual: Malicious|  {m['fn']:<5}   |  {m['tp']:<5}  (True Positives)")
    print("-----------------------------------")


def main():
    """Main function to load and evaluate results."""
    
    if not os.path.isfile(CSV_PATH):
        print(f"Error: 'results.csv' not found at {CSV_PATH}")
        print("Please run 'run_evaluation.py' first to generate the results file.")
        return
        
    try:
        df = pd.read_csv(CSV_PATH)
    except Exception as e:
        print(f"Error: Could not read CSV file at {CSV_PATH}: {e}")
        return
        
    if 'threat_score' not in df.columns or 'label' not in df.columns:
        print("Error: CSV file must contain 'threat_score' and 'label' columns.")
        return
        
    analyze_results(df)

if __name__ == "__main__":
    main()

